{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Voice gender recognition (DNN)","metadata":{}},{"cell_type":"markdown","source":"Below is the project of building voice gender recognition model(using DNN) and finding the best parameter regularization and the best accuracy.","metadata":{}},{"cell_type":"markdown","source":"Importing libraries","metadata":{}},{"cell_type":"code","source":"#!pip install dataprep","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:02:21.395445Z","iopub.execute_input":"2021-07-27T16:02:21.395824Z","iopub.status.idle":"2021-07-27T16:03:19.911226Z","shell.execute_reply.started":"2021-07-27T16:02:21.395744Z","shell.execute_reply":"2021-07-27T16:03:19.909998Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f53a2275a90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/dataprep/\u001b[0m\n\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f53a2275e10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/dataprep/\u001b[0m\n^C\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.regularizers import l1, l2\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import ReduceLROnPlateau\n#from dataprep.eda import plot\n\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:06:24.427841Z","iopub.execute_input":"2021-07-27T16:06:24.428272Z","iopub.status.idle":"2021-07-27T16:06:24.527093Z","shell.execute_reply.started":"2021-07-27T16:06:24.428210Z","shell.execute_reply":"2021-07-27T16:06:24.526057Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Loading the dataframe","metadata":{}},{"cell_type":"code","source":"#read input data using pandas\n#can use ../ to indicate the file in the path that it exists in\ndata = pd.read_csv('../input/voicegender/voice.csv')\n\n#display the data in csv\ndata","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-27T16:03:50.251404Z","iopub.execute_input":"2021-07-27T16:03:50.251910Z","iopub.status.idle":"2021-07-27T16:03:50.362920Z","shell.execute_reply.started":"2021-07-27T16:03:50.251866Z","shell.execute_reply":"2021-07-27T16:03:50.362054Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"      meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n0     0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n1     0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n2     0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n3     0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n4     0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n...        ...       ...       ...       ...       ...       ...        ...   \n3163  0.131884  0.084734  0.153707  0.049285  0.201144  0.151859   1.762129   \n3164  0.116221  0.089221  0.076758  0.042718  0.204911  0.162193   0.693730   \n3165  0.142056  0.095798  0.183731  0.033424  0.224360  0.190936   1.876502   \n3166  0.143659  0.090628  0.184976  0.043508  0.219943  0.176435   1.591065   \n3167  0.165509  0.092884  0.183044  0.070072  0.250827  0.180756   1.705029   \n\n             kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n0      274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n1      634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n2     1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n3        4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n4        4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n...           ...       ...       ...  ...       ...       ...       ...   \n3163     6.630383  0.962934  0.763182  ...  0.131884  0.182790  0.083770   \n3164     2.503954  0.960716  0.709570  ...  0.116221  0.188980  0.034409   \n3165     6.604509  0.946854  0.654196  ...  0.142056  0.209918  0.039506   \n3166     5.388298  0.950436  0.675470  ...  0.143659  0.172375  0.034483   \n3167     5.769115  0.938829  0.601529  ...  0.165509  0.185607  0.062257   \n\n        maxfun   meandom    mindom    maxdom   dfrange   modindx   label  \n0     0.275862  0.007812  0.007812  0.007812  0.000000  0.000000    male  \n1     0.250000  0.009014  0.007812  0.054688  0.046875  0.052632    male  \n2     0.271186  0.007990  0.007812  0.015625  0.007812  0.046512    male  \n3     0.250000  0.201497  0.007812  0.562500  0.554688  0.247119    male  \n4     0.266667  0.712812  0.007812  5.484375  5.476562  0.208274    male  \n...        ...       ...       ...       ...       ...       ...     ...  \n3163  0.262295  0.832899  0.007812  4.210938  4.203125  0.161929  female  \n3164  0.275862  0.909856  0.039062  3.679688  3.640625  0.277897  female  \n3165  0.275862  0.494271  0.007812  2.937500  2.929688  0.194759  female  \n3166  0.250000  0.791360  0.007812  3.593750  3.585938  0.311002  female  \n3167  0.271186  0.227022  0.007812  0.554688  0.546875  0.350000  female  \n\n[3168 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meanfreq</th>\n      <th>sd</th>\n      <th>median</th>\n      <th>Q25</th>\n      <th>Q75</th>\n      <th>IQR</th>\n      <th>skew</th>\n      <th>kurt</th>\n      <th>sp.ent</th>\n      <th>sfm</th>\n      <th>...</th>\n      <th>centroid</th>\n      <th>meanfun</th>\n      <th>minfun</th>\n      <th>maxfun</th>\n      <th>meandom</th>\n      <th>mindom</th>\n      <th>maxdom</th>\n      <th>dfrange</th>\n      <th>modindx</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.059781</td>\n      <td>0.064241</td>\n      <td>0.032027</td>\n      <td>0.015071</td>\n      <td>0.090193</td>\n      <td>0.075122</td>\n      <td>12.863462</td>\n      <td>274.402906</td>\n      <td>0.893369</td>\n      <td>0.491918</td>\n      <td>...</td>\n      <td>0.059781</td>\n      <td>0.084279</td>\n      <td>0.015702</td>\n      <td>0.275862</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.066009</td>\n      <td>0.067310</td>\n      <td>0.040229</td>\n      <td>0.019414</td>\n      <td>0.092666</td>\n      <td>0.073252</td>\n      <td>22.423285</td>\n      <td>634.613855</td>\n      <td>0.892193</td>\n      <td>0.513724</td>\n      <td>...</td>\n      <td>0.066009</td>\n      <td>0.107937</td>\n      <td>0.015826</td>\n      <td>0.250000</td>\n      <td>0.009014</td>\n      <td>0.007812</td>\n      <td>0.054688</td>\n      <td>0.046875</td>\n      <td>0.052632</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.077316</td>\n      <td>0.083829</td>\n      <td>0.036718</td>\n      <td>0.008701</td>\n      <td>0.131908</td>\n      <td>0.123207</td>\n      <td>30.757155</td>\n      <td>1024.927705</td>\n      <td>0.846389</td>\n      <td>0.478905</td>\n      <td>...</td>\n      <td>0.077316</td>\n      <td>0.098706</td>\n      <td>0.015656</td>\n      <td>0.271186</td>\n      <td>0.007990</td>\n      <td>0.007812</td>\n      <td>0.015625</td>\n      <td>0.007812</td>\n      <td>0.046512</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.151228</td>\n      <td>0.072111</td>\n      <td>0.158011</td>\n      <td>0.096582</td>\n      <td>0.207955</td>\n      <td>0.111374</td>\n      <td>1.232831</td>\n      <td>4.177296</td>\n      <td>0.963322</td>\n      <td>0.727232</td>\n      <td>...</td>\n      <td>0.151228</td>\n      <td>0.088965</td>\n      <td>0.017798</td>\n      <td>0.250000</td>\n      <td>0.201497</td>\n      <td>0.007812</td>\n      <td>0.562500</td>\n      <td>0.554688</td>\n      <td>0.247119</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.135120</td>\n      <td>0.079146</td>\n      <td>0.124656</td>\n      <td>0.078720</td>\n      <td>0.206045</td>\n      <td>0.127325</td>\n      <td>1.101174</td>\n      <td>4.333713</td>\n      <td>0.971955</td>\n      <td>0.783568</td>\n      <td>...</td>\n      <td>0.135120</td>\n      <td>0.106398</td>\n      <td>0.016931</td>\n      <td>0.266667</td>\n      <td>0.712812</td>\n      <td>0.007812</td>\n      <td>5.484375</td>\n      <td>5.476562</td>\n      <td>0.208274</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3163</th>\n      <td>0.131884</td>\n      <td>0.084734</td>\n      <td>0.153707</td>\n      <td>0.049285</td>\n      <td>0.201144</td>\n      <td>0.151859</td>\n      <td>1.762129</td>\n      <td>6.630383</td>\n      <td>0.962934</td>\n      <td>0.763182</td>\n      <td>...</td>\n      <td>0.131884</td>\n      <td>0.182790</td>\n      <td>0.083770</td>\n      <td>0.262295</td>\n      <td>0.832899</td>\n      <td>0.007812</td>\n      <td>4.210938</td>\n      <td>4.203125</td>\n      <td>0.161929</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>3164</th>\n      <td>0.116221</td>\n      <td>0.089221</td>\n      <td>0.076758</td>\n      <td>0.042718</td>\n      <td>0.204911</td>\n      <td>0.162193</td>\n      <td>0.693730</td>\n      <td>2.503954</td>\n      <td>0.960716</td>\n      <td>0.709570</td>\n      <td>...</td>\n      <td>0.116221</td>\n      <td>0.188980</td>\n      <td>0.034409</td>\n      <td>0.275862</td>\n      <td>0.909856</td>\n      <td>0.039062</td>\n      <td>3.679688</td>\n      <td>3.640625</td>\n      <td>0.277897</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>3165</th>\n      <td>0.142056</td>\n      <td>0.095798</td>\n      <td>0.183731</td>\n      <td>0.033424</td>\n      <td>0.224360</td>\n      <td>0.190936</td>\n      <td>1.876502</td>\n      <td>6.604509</td>\n      <td>0.946854</td>\n      <td>0.654196</td>\n      <td>...</td>\n      <td>0.142056</td>\n      <td>0.209918</td>\n      <td>0.039506</td>\n      <td>0.275862</td>\n      <td>0.494271</td>\n      <td>0.007812</td>\n      <td>2.937500</td>\n      <td>2.929688</td>\n      <td>0.194759</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>3166</th>\n      <td>0.143659</td>\n      <td>0.090628</td>\n      <td>0.184976</td>\n      <td>0.043508</td>\n      <td>0.219943</td>\n      <td>0.176435</td>\n      <td>1.591065</td>\n      <td>5.388298</td>\n      <td>0.950436</td>\n      <td>0.675470</td>\n      <td>...</td>\n      <td>0.143659</td>\n      <td>0.172375</td>\n      <td>0.034483</td>\n      <td>0.250000</td>\n      <td>0.791360</td>\n      <td>0.007812</td>\n      <td>3.593750</td>\n      <td>3.585938</td>\n      <td>0.311002</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>3167</th>\n      <td>0.165509</td>\n      <td>0.092884</td>\n      <td>0.183044</td>\n      <td>0.070072</td>\n      <td>0.250827</td>\n      <td>0.180756</td>\n      <td>1.705029</td>\n      <td>5.769115</td>\n      <td>0.938829</td>\n      <td>0.601529</td>\n      <td>...</td>\n      <td>0.165509</td>\n      <td>0.185607</td>\n      <td>0.062257</td>\n      <td>0.271186</td>\n      <td>0.227022</td>\n      <td>0.007812</td>\n      <td>0.554688</td>\n      <td>0.546875</td>\n      <td>0.350000</td>\n      <td>female</td>\n    </tr>\n  </tbody>\n</table>\n<p>3168 rows Ã— 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We don't have a lot of objects in the dataset and I want to use a DNN. For reducing overfitting let's find the best regularization parameter and find the best accuracy then let's use DNN with Dropout. It is different techniques of regularization and interesting compare accuracies.","metadata":{}},{"cell_type":"markdown","source":"First of all, let's analyze the data using seaborn library","metadata":{}},{"cell_type":"code","source":"#plot the labeled data, there seems to be equal male and female data\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x = \"label\", data = data)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:11:13.646451Z","iopub.execute_input":"2021-07-27T16:11:13.646857Z","iopub.status.idle":"2021-07-27T16:11:13.794887Z","shell.execute_reply.started":"2021-07-27T16:11:13.646822Z","shell.execute_reply":"2021-07-27T16:11:13.793779Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":"We have equal objects labeled as female and male ","metadata":{}},{"cell_type":"markdown","source":"Splitting the data into train and test","metadata":{}},{"cell_type":"code","source":"#y = labels, putting the labels into a numpy array\ny = data['label'].copy()\ny = np.array(y)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:30:29.122076Z","iopub.execute_input":"2021-07-27T16:30:29.122882Z","iopub.status.idle":"2021-07-27T16:30:29.131956Z","shell.execute_reply.started":"2021-07-27T16:30:29.122829Z","shell.execute_reply":"2021-07-27T16:30:29.131098Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#replacing all male labels with 0 and female labels with 1\ny = np.where(y == 'male', 0, y)\ny = np.where(y == 'female', 1, y)\ny = np.asarray(y).astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:30:31.100245Z","iopub.execute_input":"2021-07-27T16:30:31.100612Z","iopub.status.idle":"2021-07-27T16:30:31.107877Z","shell.execute_reply.started":"2021-07-27T16:30:31.100580Z","shell.execute_reply":"2021-07-27T16:30:31.106333Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#features without the labels column\nx = data.drop('label', axis=1).copy()\n\n#puts all the data into a numpy array\nx = np.array(x, dtype='float32')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:30:33.209436Z","iopub.execute_input":"2021-07-27T16:30:33.209817Z","iopub.status.idle":"2021-07-27T16:30:33.218594Z","shell.execute_reply.started":"2021-07-27T16:30:33.209778Z","shell.execute_reply":"2021-07-27T16:30:33.217536Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#split into test and training\n(trainX, testX, trainY, testY) = train_test_split(x,\n    y, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:30:35.195357Z","iopub.execute_input":"2021-07-27T16:30:35.195745Z","iopub.status.idle":"2021-07-27T16:30:35.204767Z","shell.execute_reply.started":"2021-07-27T16:30:35.195710Z","shell.execute_reply":"2021-07-27T16:30:35.203681Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#array of regularizer values to be tested later to find the best parameter for reducing overfit\nregul = [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n\n#for experimentation + documentation, list that holds the accuracy for each experiment\nall_train, all_test = list(), list()\nmodels = list()\n\n#training hyperparameters\n#learning rate\nINIT_LR = 0.0002\nEPOCHS = 150\nBATCH = 8","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:30:36.810712Z","iopub.execute_input":"2021-07-27T16:30:36.811092Z","iopub.status.idle":"2021-07-27T16:30:36.817327Z","shell.execute_reply.started":"2021-07-27T16:30:36.811057Z","shell.execute_reply":"2021-07-27T16:30:36.816053Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Finding best regularization parameter for reducing overfitting","metadata":{}},{"cell_type":"markdown","source":"Let's build models with different regularization parameters.","metadata":{}},{"cell_type":"code","source":"#for loop, builds a model for each of the regularizer parameters\nfor param in regul:\n    # define model\n    model = Sequential()\n    model.add(tf.keras.layers.Dense(input_shape=(trainX.shape[1],), units=256, \n                                activation=\"relu\", kernel_regularizer=l2(param)))\n    model.add(tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=l2(param)))\n    model.add(tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(param)))\n    model.add(tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(param)))\n    model.add(tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=l2(param)))\n    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n    #compile the model using Adam as an optimizer and \n    \n    #a binary cross entropy as a loss function\n    print(\"[INFO] training network...\")\n    opt = Adam(lr=INIT_LR)\n    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n    \n    #callbacks = operations to perform during learning to improve performance, control learning\n    #reduceLRonPlateau = reduce learning rate by 1/2 if loss does not improve after 5 epochs. do not let lr go under 0.00001\n    callbacks = [ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=0.00001)] \n    \n    #fit function, history created\n    #stores the metrics that are created when the model runs during training\n    history_model = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH, callbacks=callbacks, verbose=1)\n    \n    #add history to the array\n    models.append(model)\n    \n    # evaluate the model using train and test data, loss and accuract\n    #_ means you don't care about the first output, only keep the second output from the function\n    _, train_acc = model.evaluate(trainX, trainY, verbose=0)\n    _, test_acc = model.evaluate(testX, testY, verbose=0)\n    print('Param: %f, Train: %.3f, Test: %.3f' % (param, train_acc, test_acc))\n    all_train.append(train_acc)\n    all_test.append(test_acc)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:30:38.900997Z","iopub.execute_input":"2021-07-27T16:30:38.901376Z","iopub.status.idle":"2021-07-27T16:39:05.699619Z","shell.execute_reply.started":"2021-07-27T16:30:38.901343Z","shell.execute_reply":"2021-07-27T16:39:05.698595Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[INFO] training network...\nEpoch 1/150\n297/297 [==============================] - 1s 2ms/step - loss: 4.1438 - accuracy: 0.5024\nEpoch 2/150\n297/297 [==============================] - 0s 2ms/step - loss: 2.6051 - accuracy: 0.5474\nEpoch 3/150\n297/297 [==============================] - 0s 2ms/step - loss: 2.0031 - accuracy: 0.6145\nEpoch 4/150\n297/297 [==============================] - 1s 2ms/step - loss: 1.7291 - accuracy: 0.6182\nEpoch 5/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.5579 - accuracy: 0.6641\nEpoch 6/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.4524 - accuracy: 0.6567\nEpoch 7/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.3583 - accuracy: 0.6645\nEpoch 8/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.2749 - accuracy: 0.6749\nEpoch 9/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.2022 - accuracy: 0.6751\nEpoch 10/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.1312 - accuracy: 0.6841\nEpoch 11/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.0734 - accuracy: 0.6957\nEpoch 12/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.0277 - accuracy: 0.6923\nEpoch 13/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.9780 - accuracy: 0.6951\nEpoch 14/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.9329 - accuracy: 0.7114\nEpoch 15/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.9181 - accuracy: 0.6987\nEpoch 16/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.8777 - accuracy: 0.7000\nEpoch 17/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.8484 - accuracy: 0.7184\nEpoch 18/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.8319 - accuracy: 0.7151\nEpoch 19/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7990 - accuracy: 0.7215\nEpoch 20/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.8059 - accuracy: 0.6934\nEpoch 21/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7685 - accuracy: 0.7227\nEpoch 22/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7588 - accuracy: 0.7217\nEpoch 23/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7571 - accuracy: 0.7205\nEpoch 24/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7370 - accuracy: 0.7293\nEpoch 25/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7285 - accuracy: 0.7260\nEpoch 26/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7183 - accuracy: 0.7456\nEpoch 27/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.7280 - accuracy: 0.7000\nEpoch 28/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.7134\nEpoch 29/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7013 - accuracy: 0.7237\nEpoch 30/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6762 - accuracy: 0.7522\nEpoch 31/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6754 - accuracy: 0.7482\nEpoch 32/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6775 - accuracy: 0.7421\nEpoch 33/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.7454\nEpoch 34/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6579 - accuracy: 0.7471\nEpoch 35/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6561 - accuracy: 0.7475\nEpoch 36/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6391 - accuracy: 0.7518\nEpoch 37/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6368 - accuracy: 0.7514\nEpoch 38/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6414 - accuracy: 0.7535\nEpoch 39/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6434 - accuracy: 0.7455\nEpoch 40/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6347 - accuracy: 0.7527\nEpoch 41/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6285 - accuracy: 0.7672\nEpoch 42/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6186 - accuracy: 0.7605\nEpoch 43/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6171 - accuracy: 0.7631\nEpoch 44/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.6107 - accuracy: 0.7638\nEpoch 45/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.6318 - accuracy: 0.7533\nEpoch 46/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6225 - accuracy: 0.7734\nEpoch 47/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.6173 - accuracy: 0.7765\nEpoch 48/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.5937 - accuracy: 0.7812\nEpoch 49/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.6133 - accuracy: 0.7631\nEpoch 50/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.5819 - accuracy: 0.7902\nEpoch 51/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5985 - accuracy: 0.7886\nEpoch 52/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5979 - accuracy: 0.7769\nEpoch 53/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5776 - accuracy: 0.7973\nEpoch 54/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.7977\nEpoch 55/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5542 - accuracy: 0.8071\nEpoch 56/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.8020\nEpoch 57/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5730 - accuracy: 0.8134\nEpoch 58/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5530 - accuracy: 0.8165\nEpoch 59/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5745 - accuracy: 0.8072\nEpoch 60/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5706 - accuracy: 0.8182\nEpoch 61/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5485 - accuracy: 0.8242\nEpoch 62/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.8512\nEpoch 63/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.8206\nEpoch 64/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5402 - accuracy: 0.8488\nEpoch 65/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5434 - accuracy: 0.8346\nEpoch 66/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5452 - accuracy: 0.8337\nEpoch 67/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.8465\nEpoch 68/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.8414\nEpoch 69/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.8469\nEpoch 70/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5173 - accuracy: 0.8578\nEpoch 71/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.5185 - accuracy: 0.8495\nEpoch 72/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5077 - accuracy: 0.8599\nEpoch 73/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.8689\nEpoch 74/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5245 - accuracy: 0.8439\nEpoch 75/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5060 - accuracy: 0.8615\nEpoch 76/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.8695\nEpoch 77/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4986 - accuracy: 0.8678\nEpoch 78/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5184 - accuracy: 0.8485\nEpoch 79/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.8774\nEpoch 80/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4972 - accuracy: 0.8728\nEpoch 81/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.8643\nEpoch 82/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.5184 - accuracy: 0.8589\nEpoch 83/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4688 - accuracy: 0.8903\nEpoch 84/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4876 - accuracy: 0.8755\nEpoch 85/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.8757\nEpoch 86/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.8619\nEpoch 87/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.8859\nEpoch 88/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4772 - accuracy: 0.8821\nEpoch 89/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.8826\nEpoch 90/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.8798\nEpoch 91/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.8830\nEpoch 92/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.8762\nEpoch 93/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4680 - accuracy: 0.8711\nEpoch 94/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.8716\nEpoch 95/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4451 - accuracy: 0.8854\nEpoch 96/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4187 - accuracy: 0.9086\nEpoch 97/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.9124\nEpoch 98/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.9092\nEpoch 99/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4115 - accuracy: 0.9142\nEpoch 100/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4217 - accuracy: 0.9063\nEpoch 101/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3988 - accuracy: 0.9178\nEpoch 102/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4021 - accuracy: 0.9096\nEpoch 103/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4408 - accuracy: 0.8997\nEpoch 104/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4035 - accuracy: 0.9205\nEpoch 105/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.9250\nEpoch 106/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.9283\nEpoch 107/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3765 - accuracy: 0.9342\nEpoch 108/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.9310\nEpoch 109/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.9345\nEpoch 110/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.9281\nEpoch 111/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.9363\nEpoch 112/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.9210\nEpoch 113/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3780 - accuracy: 0.9275\nEpoch 114/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3572 - accuracy: 0.9355\nEpoch 115/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3531 - accuracy: 0.9385\nEpoch 116/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3542 - accuracy: 0.9473\nEpoch 117/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3590 - accuracy: 0.9371\nEpoch 118/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.9307\nEpoch 119/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.9360\nEpoch 120/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3758 - accuracy: 0.9209\nEpoch 121/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.9334\nEpoch 122/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3726 - accuracy: 0.9331\nEpoch 123/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3777 - accuracy: 0.9308\nEpoch 124/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3391 - accuracy: 0.9524\nEpoch 125/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.9501\nEpoch 126/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.9429\nEpoch 127/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.9526\nEpoch 128/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.9321\nEpoch 129/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3575 - accuracy: 0.9414\nEpoch 130/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.9406\nEpoch 131/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.9441\nEpoch 132/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3803 - accuracy: 0.9215\nEpoch 133/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.9442\nEpoch 134/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.9570\nEpoch 135/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3144 - accuracy: 0.9573\nEpoch 136/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.9572\nEpoch 137/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.9530\nEpoch 138/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3084 - accuracy: 0.9572\nEpoch 139/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.9544\nEpoch 140/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3176 - accuracy: 0.9604\nEpoch 141/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.9504\nEpoch 142/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.9497\nEpoch 143/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.9617\nEpoch 144/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3053 - accuracy: 0.9554\nEpoch 145/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.9686\nEpoch 146/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2992 - accuracy: 0.9665\nEpoch 147/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.9534\nEpoch 148/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3131 - accuracy: 0.9540\nEpoch 149/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.9632\nEpoch 150/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.9689\nParam: 0.010000, Train: 0.961, Test: 0.970\n[INFO] training network...\nEpoch 1/150\n297/297 [==============================] - 1s 2ms/step - loss: 2.4558 - accuracy: 0.5277\nEpoch 2/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.6346 - accuracy: 0.6146\nEpoch 3/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.3323 - accuracy: 0.6411\nEpoch 4/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.2005 - accuracy: 0.6980\nEpoch 5/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.1223 - accuracy: 0.7032\nEpoch 6/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.0710 - accuracy: 0.7226\nEpoch 7/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.0212 - accuracy: 0.7119\nEpoch 8/150\n297/297 [==============================] - 0s 2ms/step - loss: 1.0053 - accuracy: 0.7264\nEpoch 9/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.9865 - accuracy: 0.7278\nEpoch 10/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.9476 - accuracy: 0.7421\nEpoch 11/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.9130 - accuracy: 0.7400\nEpoch 12/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.9069 - accuracy: 0.7264\nEpoch 13/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.8883 - accuracy: 0.7376\nEpoch 14/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.8444 - accuracy: 0.7584\nEpoch 15/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.8255 - accuracy: 0.7703\nEpoch 16/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.8273 - accuracy: 0.7571\nEpoch 17/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7888 - accuracy: 0.7649\nEpoch 18/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7850 - accuracy: 0.7652\nEpoch 19/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.8012 - accuracy: 0.7594\nEpoch 20/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7600 - accuracy: 0.7760\nEpoch 21/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7209 - accuracy: 0.7918\nEpoch 22/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7070 - accuracy: 0.7961\nEpoch 23/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6955 - accuracy: 0.7989\nEpoch 24/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6828 - accuracy: 0.8111\nEpoch 25/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6687 - accuracy: 0.8015\nEpoch 26/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6647 - accuracy: 0.8120\nEpoch 27/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6362 - accuracy: 0.8240\nEpoch 28/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6486 - accuracy: 0.8187\nEpoch 29/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6254 - accuracy: 0.8275\nEpoch 30/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6055 - accuracy: 0.8308\nEpoch 31/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6051 - accuracy: 0.8432\nEpoch 32/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.5949 - accuracy: 0.8441\nEpoch 33/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.8369\nEpoch 34/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.8376\nEpoch 35/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5938 - accuracy: 0.8321\nEpoch 36/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5446 - accuracy: 0.8687\nEpoch 37/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5437 - accuracy: 0.8677\nEpoch 38/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.8665\nEpoch 39/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5708 - accuracy: 0.8478\nEpoch 40/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5590 - accuracy: 0.8546\nEpoch 41/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.8604\nEpoch 42/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5021 - accuracy: 0.8881\nEpoch 43/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.8738\nEpoch 44/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5076 - accuracy: 0.8767\nEpoch 45/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.8818\nEpoch 46/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.8659\nEpoch 47/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.8868\nEpoch 48/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4949 - accuracy: 0.8787\nEpoch 49/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4692 - accuracy: 0.8905\nEpoch 50/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.8822\nEpoch 51/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4783 - accuracy: 0.8889\nEpoch 52/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.8964\nEpoch 53/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4750 - accuracy: 0.8833\nEpoch 54/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4644 - accuracy: 0.8988\nEpoch 55/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4875 - accuracy: 0.8836\nEpoch 56/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4538 - accuracy: 0.8957\nEpoch 57/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4486 - accuracy: 0.8993\nEpoch 58/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4529 - accuracy: 0.8936\nEpoch 59/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4626 - accuracy: 0.8824\nEpoch 60/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.8876\nEpoch 61/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4294 - accuracy: 0.9100\nEpoch 62/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4229 - accuracy: 0.9093\nEpoch 63/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.8938\nEpoch 64/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4510 - accuracy: 0.8892\nEpoch 65/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4387 - accuracy: 0.8881\nEpoch 66/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.9082\nEpoch 67/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4192 - accuracy: 0.8973\nEpoch 68/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4050 - accuracy: 0.9060\nEpoch 69/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4254 - accuracy: 0.8991\nEpoch 70/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4064 - accuracy: 0.9093: 0s - loss: 0.4450 - \nEpoch 71/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.9245\nEpoch 72/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3745 - accuracy: 0.9196\nEpoch 73/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3727 - accuracy: 0.9261\nEpoch 74/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3528 - accuracy: 0.9227\nEpoch 75/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.9394\nEpoch 76/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.9182\nEpoch 77/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3554 - accuracy: 0.9236\nEpoch 78/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.9433\nEpoch 79/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.9162\nEpoch 80/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.9127\nEpoch 81/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.9465\nEpoch 82/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3308 - accuracy: 0.9409\nEpoch 83/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3141 - accuracy: 0.9476\nEpoch 84/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3293 - accuracy: 0.9440\nEpoch 85/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.9416\nEpoch 86/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.9550\nEpoch 87/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.9518\nEpoch 88/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.9484\nEpoch 89/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3085 - accuracy: 0.9406\nEpoch 90/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3176 - accuracy: 0.9423\nEpoch 91/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2903 - accuracy: 0.9611\nEpoch 92/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.9630\nEpoch 93/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.9657\nEpoch 94/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.9694\nEpoch 95/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2955 - accuracy: 0.9500\nEpoch 96/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2919 - accuracy: 0.9555\nEpoch 97/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2724 - accuracy: 0.9650\nEpoch 98/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2781 - accuracy: 0.9610\nEpoch 99/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2753 - accuracy: 0.9642\nEpoch 100/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2727 - accuracy: 0.9655\nEpoch 101/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.9579\nEpoch 102/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.9565\nEpoch 103/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2885 - accuracy: 0.9581\nEpoch 104/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.9490\nEpoch 105/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3843 - accuracy: 0.9201\nEpoch 106/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.9610\nEpoch 107/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.9637\nEpoch 108/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2798 - accuracy: 0.9549\nEpoch 109/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2685 - accuracy: 0.9686\nEpoch 110/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2658 - accuracy: 0.9649\nEpoch 111/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.9577\nEpoch 112/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.9584\nEpoch 113/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2643 - accuracy: 0.9667\nEpoch 114/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.9605\nEpoch 115/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.9618\nEpoch 116/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2698 - accuracy: 0.9647\nEpoch 117/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.9641\nEpoch 118/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.9631\nEpoch 119/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2545 - accuracy: 0.9684\nEpoch 120/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2534 - accuracy: 0.9708\nEpoch 121/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2824 - accuracy: 0.9544\nEpoch 122/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.9685\nEpoch 123/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2509 - accuracy: 0.9710\nEpoch 124/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.9673\nEpoch 125/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2475 - accuracy: 0.9746\nEpoch 126/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2493 - accuracy: 0.9720\nEpoch 127/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.9640\nEpoch 128/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2396 - accuracy: 0.9724\nEpoch 129/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2531 - accuracy: 0.9682\nEpoch 130/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2468 - accuracy: 0.9731\nEpoch 131/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2432 - accuracy: 0.9766\nEpoch 132/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2593 - accuracy: 0.9629\nEpoch 133/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2514 - accuracy: 0.9663\nEpoch 134/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2532 - accuracy: 0.9658\nEpoch 135/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.9679\nEpoch 136/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2664 - accuracy: 0.9640\nEpoch 137/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 0.9758\nEpoch 138/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2581 - accuracy: 0.9668\nEpoch 139/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2505 - accuracy: 0.9728\nEpoch 140/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.9740\nEpoch 141/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2525 - accuracy: 0.9736\nEpoch 142/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2463 - accuracy: 0.9705\nEpoch 143/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2466 - accuracy: 0.9703\nEpoch 144/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2523 - accuracy: 0.9680\nEpoch 145/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2505 - accuracy: 0.9693\nEpoch 146/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2357 - accuracy: 0.9760\nEpoch 147/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.9645\nEpoch 148/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9717\nEpoch 149/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.9723\nEpoch 150/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2498 - accuracy: 0.9685\nParam: 0.005000, Train: 0.971, Test: 0.972\n[INFO] training network...\nEpoch 1/150\n297/297 [==============================] - 1s 2ms/step - loss: 1.1013 - accuracy: 0.5190\nEpoch 2/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.9442 - accuracy: 0.6263\nEpoch 3/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.8242 - accuracy: 0.6567\nEpoch 4/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7735 - accuracy: 0.6982\nEpoch 5/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7294 - accuracy: 0.7210\nEpoch 6/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7313 - accuracy: 0.7296\nEpoch 7/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7042 - accuracy: 0.7441\nEpoch 8/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.7635\nEpoch 9/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6791 - accuracy: 0.7576\nEpoch 10/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.7789\nEpoch 11/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6105 - accuracy: 0.8128\nEpoch 12/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5853 - accuracy: 0.8312\nEpoch 13/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5806 - accuracy: 0.7998\nEpoch 14/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5732 - accuracy: 0.8254\nEpoch 15/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5891 - accuracy: 0.8243\nEpoch 16/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5398 - accuracy: 0.8511\nEpoch 17/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.5251 - accuracy: 0.8509\nEpoch 18/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.8567\nEpoch 19/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.8545\nEpoch 20/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5318 - accuracy: 0.8449\nEpoch 21/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4783 - accuracy: 0.8582\nEpoch 22/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4527 - accuracy: 0.8798\nEpoch 23/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.8666\nEpoch 24/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.8922\nEpoch 25/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.8500\nEpoch 26/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.8832\nEpoch 27/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4353 - accuracy: 0.8854\nEpoch 28/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.8854\nEpoch 29/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.8887\nEpoch 30/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.9022\nEpoch 31/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4432 - accuracy: 0.8815\nEpoch 32/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3909 - accuracy: 0.9010\nEpoch 33/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3855 - accuracy: 0.8988\nEpoch 34/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.9065\nEpoch 35/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3590 - accuracy: 0.9179\nEpoch 36/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3757 - accuracy: 0.9045\nEpoch 37/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.9085\nEpoch 38/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.9167\nEpoch 39/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3435 - accuracy: 0.9258\nEpoch 40/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.9081\nEpoch 41/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3516 - accuracy: 0.9092\nEpoch 42/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3469 - accuracy: 0.9164\nEpoch 43/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.9116\nEpoch 44/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3245 - accuracy: 0.9227\nEpoch 45/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.9157\nEpoch 46/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3495 - accuracy: 0.9029\nEpoch 47/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.9136\nEpoch 48/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3256 - accuracy: 0.9260\nEpoch 49/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3390 - accuracy: 0.9195\nEpoch 50/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3383 - accuracy: 0.9190\nEpoch 51/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3201 - accuracy: 0.9237\nEpoch 52/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.9124\nEpoch 53/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.9174\nEpoch 54/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.9212\nEpoch 55/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3034 - accuracy: 0.9255\nEpoch 56/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3254 - accuracy: 0.9222\nEpoch 57/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.9142\nEpoch 58/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3126 - accuracy: 0.9225\nEpoch 59/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2987 - accuracy: 0.9296\nEpoch 60/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.9210\nEpoch 61/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3005 - accuracy: 0.9253\nEpoch 62/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2864 - accuracy: 0.9362\nEpoch 63/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3236 - accuracy: 0.9218\nEpoch 64/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2909 - accuracy: 0.9342\nEpoch 65/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2981 - accuracy: 0.9305\nEpoch 66/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3021 - accuracy: 0.9235\nEpoch 67/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2876 - accuracy: 0.9301\nEpoch 68/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.9201\nEpoch 69/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2887 - accuracy: 0.9312\nEpoch 70/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2979 - accuracy: 0.9208\nEpoch 71/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2903 - accuracy: 0.9344\nEpoch 72/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3142 - accuracy: 0.9234\nEpoch 73/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2752 - accuracy: 0.9423\nEpoch 74/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.9284\nEpoch 75/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2769 - accuracy: 0.9398\nEpoch 76/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2598 - accuracy: 0.9429\nEpoch 77/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.9387\nEpoch 78/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2721 - accuracy: 0.9341\nEpoch 79/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2773 - accuracy: 0.9369\nEpoch 80/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2415 - accuracy: 0.9506\nEpoch 81/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2652 - accuracy: 0.9378\nEpoch 82/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.9424\nEpoch 83/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2386 - accuracy: 0.9492\nEpoch 84/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2420 - accuracy: 0.9584\nEpoch 85/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2526 - accuracy: 0.9462\nEpoch 86/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2582 - accuracy: 0.9406\nEpoch 87/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2498 - accuracy: 0.9462\nEpoch 88/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.9363\nEpoch 89/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2685 - accuracy: 0.9433\nEpoch 90/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.9478\nEpoch 91/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2426 - accuracy: 0.9516\nEpoch 92/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.9467\nEpoch 93/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2382 - accuracy: 0.9556\nEpoch 94/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2416 - accuracy: 0.9454\nEpoch 95/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2433 - accuracy: 0.9410\nEpoch 96/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2509 - accuracy: 0.9346\nEpoch 97/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.9534\nEpoch 98/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2345 - accuracy: 0.9482\nEpoch 99/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2261 - accuracy: 0.9581\nEpoch 100/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9465\nEpoch 101/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2440 - accuracy: 0.9508\nEpoch 102/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2432 - accuracy: 0.9469\nEpoch 103/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2441 - accuracy: 0.9431\nEpoch 104/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2281 - accuracy: 0.9474\nEpoch 105/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.9567\nEpoch 106/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2093 - accuracy: 0.9668\nEpoch 107/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2174 - accuracy: 0.9613\nEpoch 108/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2117 - accuracy: 0.9584\nEpoch 109/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2225 - accuracy: 0.9544\nEpoch 110/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2097 - accuracy: 0.9618\nEpoch 111/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2116 - accuracy: 0.9584\nEpoch 112/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2154 - accuracy: 0.9557\nEpoch 113/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2150 - accuracy: 0.9561\nEpoch 114/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2160 - accuracy: 0.9554\nEpoch 115/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2141 - accuracy: 0.9583\nEpoch 116/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2253 - accuracy: 0.9522\nEpoch 117/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2210 - accuracy: 0.9528\nEpoch 118/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2164 - accuracy: 0.9589\nEpoch 119/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2131 - accuracy: 0.9525\nEpoch 120/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1995 - accuracy: 0.9648\nEpoch 121/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2090 - accuracy: 0.9621\nEpoch 122/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2083 - accuracy: 0.9586\nEpoch 123/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1941 - accuracy: 0.9694\nEpoch 124/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2025 - accuracy: 0.9596\nEpoch 125/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2085 - accuracy: 0.9585\nEpoch 126/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2146 - accuracy: 0.9597\nEpoch 127/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9605\nEpoch 128/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2154 - accuracy: 0.9605\nEpoch 129/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2002 - accuracy: 0.9610\nEpoch 130/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2005 - accuracy: 0.9620\nEpoch 131/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2160 - accuracy: 0.9582\nEpoch 132/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.1960 - accuracy: 0.9643\nEpoch 133/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1994 - accuracy: 0.9664\nEpoch 134/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1909 - accuracy: 0.9749\nEpoch 135/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1806 - accuracy: 0.9760\nEpoch 136/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1857 - accuracy: 0.9701\nEpoch 137/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1881 - accuracy: 0.9707\nEpoch 138/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1943 - accuracy: 0.9659\nEpoch 139/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1933 - accuracy: 0.9702\nEpoch 140/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1903 - accuracy: 0.9688\nEpoch 141/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1997 - accuracy: 0.9670\nEpoch 142/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1951 - accuracy: 0.9655\nEpoch 143/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1924 - accuracy: 0.9728\nEpoch 144/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1850 - accuracy: 0.9720\nEpoch 145/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1908 - accuracy: 0.9713\nEpoch 146/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9643\nEpoch 147/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9665\nEpoch 148/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1862 - accuracy: 0.9738\nEpoch 149/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1915 - accuracy: 0.9662\nEpoch 150/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1858 - accuracy: 0.9708\nParam: 0.001000, Train: 0.968, Test: 0.952\n[INFO] training network...\nEpoch 1/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.8958 - accuracy: 0.5363\nEpoch 2/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.7883 - accuracy: 0.6421\nEpoch 3/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.6754 - accuracy: 0.6952\nEpoch 4/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6226 - accuracy: 0.7184\nEpoch 5/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6150 - accuracy: 0.7281\nEpoch 6/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5883 - accuracy: 0.7276\nEpoch 7/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5827 - accuracy: 0.7530\nEpoch 8/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.7778\nEpoch 9/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7851\nEpoch 10/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.7846\nEpoch 11/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.8079\nEpoch 12/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.8211\nEpoch 13/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4629 - accuracy: 0.8392\nEpoch 14/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.8576\nEpoch 15/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4539 - accuracy: 0.8518\nEpoch 16/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8472\nEpoch 17/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.8591\nEpoch 18/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4208 - accuracy: 0.8527\nEpoch 19/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.8714\nEpoch 20/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8637\nEpoch 21/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3762 - accuracy: 0.8809\nEpoch 22/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3537 - accuracy: 0.8955\nEpoch 23/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8678\nEpoch 24/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8799\nEpoch 25/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3424 - accuracy: 0.8920\nEpoch 26/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3293 - accuracy: 0.9041\nEpoch 27/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3389 - accuracy: 0.8929\nEpoch 28/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.9116\nEpoch 29/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3428 - accuracy: 0.8908\nEpoch 30/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.9123\nEpoch 31/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2980 - accuracy: 0.9111\nEpoch 32/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3003 - accuracy: 0.9103\nEpoch 33/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2937 - accuracy: 0.9223\nEpoch 34/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2886 - accuracy: 0.9199\nEpoch 35/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3033 - accuracy: 0.9167\nEpoch 36/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2832 - accuracy: 0.9226\nEpoch 37/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2779 - accuracy: 0.9242\nEpoch 38/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2895 - accuracy: 0.9213\nEpoch 39/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.9262\nEpoch 40/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2865 - accuracy: 0.9234\nEpoch 41/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3135 - accuracy: 0.9041\nEpoch 42/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2625 - accuracy: 0.9349\nEpoch 43/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.9203\nEpoch 44/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2635 - accuracy: 0.9353\nEpoch 45/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2789 - accuracy: 0.9205\nEpoch 46/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2814 - accuracy: 0.9213\nEpoch 47/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.9251\nEpoch 48/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.9215\nEpoch 49/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2680 - accuracy: 0.9229\nEpoch 50/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.9361\nEpoch 51/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2739 - accuracy: 0.9249\nEpoch 52/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2588 - accuracy: 0.9413\nEpoch 53/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.9232\nEpoch 54/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.9197\nEpoch 55/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.9264\nEpoch 56/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2704 - accuracy: 0.9189\nEpoch 57/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.9286\nEpoch 58/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2466 - accuracy: 0.9346\nEpoch 59/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2680 - accuracy: 0.9266\nEpoch 60/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2382 - accuracy: 0.9357\nEpoch 61/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2418 - accuracy: 0.9361\nEpoch 62/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2202 - accuracy: 0.9467\nEpoch 63/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2309 - accuracy: 0.9417\nEpoch 64/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.9390\nEpoch 65/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2494 - accuracy: 0.9395\nEpoch 66/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.9450\nEpoch 67/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2456 - accuracy: 0.9331\nEpoch 68/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2293 - accuracy: 0.9415\nEpoch 69/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2221 - accuracy: 0.9344\nEpoch 70/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2384 - accuracy: 0.9387\nEpoch 71/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2174 - accuracy: 0.9474\nEpoch 72/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2068 - accuracy: 0.9495\nEpoch 73/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2041 - accuracy: 0.9480\nEpoch 74/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2049 - accuracy: 0.9498\nEpoch 75/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2041 - accuracy: 0.9489\nEpoch 76/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1902 - accuracy: 0.9608\nEpoch 77/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1819 - accuracy: 0.9580\nEpoch 78/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1896 - accuracy: 0.9595\nEpoch 79/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1813 - accuracy: 0.9616\nEpoch 80/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1932 - accuracy: 0.9565\nEpoch 81/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1914 - accuracy: 0.9608\nEpoch 82/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1802 - accuracy: 0.9660\nEpoch 83/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1926 - accuracy: 0.9559\nEpoch 84/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1836 - accuracy: 0.9654\nEpoch 85/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1886 - accuracy: 0.9583\nEpoch 86/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1843 - accuracy: 0.9615\nEpoch 87/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1944 - accuracy: 0.9569\nEpoch 88/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1869 - accuracy: 0.9565\nEpoch 89/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1902 - accuracy: 0.9563\nEpoch 90/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1626 - accuracy: 0.9675\nEpoch 91/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1712 - accuracy: 0.9641\nEpoch 92/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1733 - accuracy: 0.9630\nEpoch 93/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1667 - accuracy: 0.9634\nEpoch 94/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1839 - accuracy: 0.9647\nEpoch 95/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1480 - accuracy: 0.9729\nEpoch 96/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.1569 - accuracy: 0.9728\nEpoch 97/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1866 - accuracy: 0.9598\nEpoch 98/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.1703 - accuracy: 0.9675\nEpoch 99/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.1595 - accuracy: 0.9731\nEpoch 100/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.1696 - accuracy: 0.9678\nEpoch 101/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1549 - accuracy: 0.9725\nEpoch 102/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1532 - accuracy: 0.9769\nEpoch 103/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1508 - accuracy: 0.9777\nEpoch 104/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1576 - accuracy: 0.9724\nEpoch 105/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1558 - accuracy: 0.9713\nEpoch 106/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9759\nEpoch 107/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1603 - accuracy: 0.9677\nEpoch 108/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1520 - accuracy: 0.9759\nEpoch 109/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1557 - accuracy: 0.9727\nEpoch 110/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1386 - accuracy: 0.9794\nEpoch 111/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1456 - accuracy: 0.9791\nEpoch 112/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1429 - accuracy: 0.9763\nEpoch 113/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1516 - accuracy: 0.9718\nEpoch 114/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1474 - accuracy: 0.9745\nEpoch 115/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1378 - accuracy: 0.9807\nEpoch 116/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1496 - accuracy: 0.9735\nEpoch 117/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1580 - accuracy: 0.9686\nEpoch 118/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1485 - accuracy: 0.9735\nEpoch 119/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.1437 - accuracy: 0.9747\nEpoch 120/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1435 - accuracy: 0.9748\nEpoch 121/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1454 - accuracy: 0.9763\nEpoch 122/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1550 - accuracy: 0.9723\nEpoch 123/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1491 - accuracy: 0.9747\nEpoch 124/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1505 - accuracy: 0.9692\nEpoch 125/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9782\nEpoch 126/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1378 - accuracy: 0.9797\nEpoch 127/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1368 - accuracy: 0.9789\nEpoch 128/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1388 - accuracy: 0.9779\nEpoch 129/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1341 - accuracy: 0.9810\nEpoch 130/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1327 - accuracy: 0.9810\nEpoch 131/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1416 - accuracy: 0.9778\nEpoch 132/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1416 - accuracy: 0.9804\nEpoch 133/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.1446 - accuracy: 0.9776\nEpoch 134/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1343 - accuracy: 0.9817\nEpoch 135/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9796\nEpoch 136/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1416 - accuracy: 0.9794\nEpoch 137/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1384 - accuracy: 0.9804\nEpoch 138/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1512 - accuracy: 0.9754\nEpoch 139/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1443 - accuracy: 0.9777\nEpoch 140/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1396 - accuracy: 0.9820\nEpoch 141/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.9781\nEpoch 142/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1352 - accuracy: 0.9824\nEpoch 143/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.9800\nEpoch 144/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1489 - accuracy: 0.9716\nEpoch 145/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1464 - accuracy: 0.9795\nEpoch 146/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1471 - accuracy: 0.9739\nEpoch 147/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1322 - accuracy: 0.9798\nEpoch 148/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1412 - accuracy: 0.9795\nEpoch 149/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1479 - accuracy: 0.9765\nEpoch 150/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1370 - accuracy: 0.9800\nParam: 0.000500, Train: 0.980, Test: 0.962\n[INFO] training network...\nEpoch 1/150\n297/297 [==============================] - ETA: 0s - loss: 0.7703 - accuracy: 0.52 - 1s 2ms/step - loss: 0.7686 - accuracy: 0.5350\nEpoch 2/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.6669\nEpoch 3/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5942 - accuracy: 0.6882\nEpoch 4/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5625 - accuracy: 0.7367\nEpoch 5/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5411 - accuracy: 0.7461\nEpoch 6/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7619\nEpoch 7/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.7879\nEpoch 8/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.8194\nEpoch 9/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4504 - accuracy: 0.8218\nEpoch 10/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.8122\nEpoch 11/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4234 - accuracy: 0.8265\nEpoch 12/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4125 - accuracy: 0.8319\nEpoch 13/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3815 - accuracy: 0.8426\nEpoch 14/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3616 - accuracy: 0.8464\nEpoch 15/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3636 - accuracy: 0.8565\nEpoch 16/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3260 - accuracy: 0.8701\nEpoch 17/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.8799\nEpoch 18/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8740\nEpoch 19/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3005 - accuracy: 0.8897\nEpoch 20/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3345 - accuracy: 0.8617\nEpoch 21/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3144 - accuracy: 0.8833\nEpoch 22/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.8935\nEpoch 23/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.8879\nEpoch 24/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2743 - accuracy: 0.8957\nEpoch 25/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2658 - accuracy: 0.9031\nEpoch 26/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2443 - accuracy: 0.9139\nEpoch 27/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.9076\nEpoch 28/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2412 - accuracy: 0.9120\nEpoch 29/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 0.9025\nEpoch 30/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2305 - accuracy: 0.9099\nEpoch 31/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2256 - accuracy: 0.9242\nEpoch 32/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2407 - accuracy: 0.9190\nEpoch 33/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2313 - accuracy: 0.9147\nEpoch 34/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2225 - accuracy: 0.9278\nEpoch 35/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2370 - accuracy: 0.9143\nEpoch 36/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2120 - accuracy: 0.9178\nEpoch 37/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2345 - accuracy: 0.9166\nEpoch 38/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2207 - accuracy: 0.9194\nEpoch 39/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2006 - accuracy: 0.9337\nEpoch 40/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2189 - accuracy: 0.9207\nEpoch 41/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2296 - accuracy: 0.9155\nEpoch 42/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2231 - accuracy: 0.9258\nEpoch 43/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1924 - accuracy: 0.9366\nEpoch 44/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2307 - accuracy: 0.9270\nEpoch 45/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2044 - accuracy: 0.9228\nEpoch 46/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2070 - accuracy: 0.9233\nEpoch 47/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1918 - accuracy: 0.9358\nEpoch 48/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.9231\nEpoch 49/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1825 - accuracy: 0.9393\nEpoch 50/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1845 - accuracy: 0.9335\nEpoch 51/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1941 - accuracy: 0.9319\nEpoch 52/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.9329\nEpoch 53/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9335\nEpoch 54/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1957 - accuracy: 0.9371\nEpoch 55/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1867 - accuracy: 0.9387\nEpoch 56/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1781 - accuracy: 0.9367\nEpoch 57/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1813 - accuracy: 0.9375\nEpoch 58/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1766 - accuracy: 0.9458\nEpoch 59/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1962 - accuracy: 0.9280\nEpoch 60/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.2144 - accuracy: 0.9275\nEpoch 61/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1525 - accuracy: 0.9498\nEpoch 62/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1693 - accuracy: 0.9479\nEpoch 63/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1669 - accuracy: 0.9422\nEpoch 64/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1466 - accuracy: 0.9556\nEpoch 65/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1565 - accuracy: 0.9525\nEpoch 66/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1542 - accuracy: 0.9549\nEpoch 67/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1546 - accuracy: 0.9471\nEpoch 68/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1736 - accuracy: 0.9360\nEpoch 69/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.9553\nEpoch 70/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1496 - accuracy: 0.9461\nEpoch 71/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1486 - accuracy: 0.9541\nEpoch 72/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1545 - accuracy: 0.9485\nEpoch 73/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1514 - accuracy: 0.9530\nEpoch 74/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1399 - accuracy: 0.9558\nEpoch 75/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1446 - accuracy: 0.9530\nEpoch 76/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1389 - accuracy: 0.9538\nEpoch 77/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1278 - accuracy: 0.9673\nEpoch 78/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9594\nEpoch 79/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1386 - accuracy: 0.9508\nEpoch 80/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1415 - accuracy: 0.9578\nEpoch 81/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1355 - accuracy: 0.9625\nEpoch 82/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9554\nEpoch 83/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.1181 - accuracy: 0.9663\nEpoch 84/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1294 - accuracy: 0.9667\nEpoch 85/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1217 - accuracy: 0.9652\nEpoch 86/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1251 - accuracy: 0.9637\nEpoch 87/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1299 - accuracy: 0.9566\nEpoch 88/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1045 - accuracy: 0.9665\nEpoch 89/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.9648\nEpoch 90/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1080 - accuracy: 0.9745\nEpoch 91/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1131 - accuracy: 0.9691\nEpoch 92/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0888 - accuracy: 0.9770\nEpoch 93/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1235 - accuracy: 0.9650\nEpoch 94/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0970 - accuracy: 0.9756\nEpoch 95/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0938 - accuracy: 0.9732\nEpoch 96/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0954 - accuracy: 0.9753\nEpoch 97/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0916 - accuracy: 0.9800\nEpoch 98/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0901 - accuracy: 0.9778\nEpoch 99/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9780\nEpoch 100/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0903 - accuracy: 0.9781\nEpoch 101/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0944 - accuracy: 0.9736\nEpoch 102/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1032 - accuracy: 0.9725\nEpoch 103/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0901 - accuracy: 0.9781\nEpoch 104/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0869 - accuracy: 0.9767\nEpoch 105/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0805 - accuracy: 0.9815\nEpoch 106/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0994 - accuracy: 0.9759\nEpoch 107/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0803 - accuracy: 0.9810\nEpoch 108/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0870 - accuracy: 0.9839\nEpoch 109/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0870 - accuracy: 0.9787\nEpoch 110/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0945 - accuracy: 0.9749\nEpoch 111/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0830 - accuracy: 0.9807\nEpoch 112/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0933 - accuracy: 0.9797\nEpoch 113/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0857 - accuracy: 0.9777\nEpoch 114/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0908 - accuracy: 0.9789\nEpoch 115/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0882 - accuracy: 0.9766\nEpoch 116/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0911 - accuracy: 0.9773\nEpoch 117/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0844 - accuracy: 0.9824\nEpoch 118/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0735 - accuracy: 0.9834\nEpoch 119/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0812 - accuracy: 0.9819\nEpoch 120/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0807 - accuracy: 0.9796\nEpoch 121/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.9838\nEpoch 122/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.9832\nEpoch 123/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0943 - accuracy: 0.9761\nEpoch 124/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0942 - accuracy: 0.9775\nEpoch 125/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0881 - accuracy: 0.9793\nEpoch 126/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0780 - accuracy: 0.9843\nEpoch 127/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0782 - accuracy: 0.9845\nEpoch 128/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0818 - accuracy: 0.9801\nEpoch 129/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.9855\nEpoch 130/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.9793\nEpoch 131/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9841\nEpoch 132/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0812 - accuracy: 0.9815\nEpoch 133/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0761 - accuracy: 0.9820\nEpoch 134/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0858 - accuracy: 0.9834\nEpoch 135/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0835 - accuracy: 0.9788\nEpoch 136/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0718 - accuracy: 0.9835\nEpoch 137/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9828\nEpoch 138/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9875\nEpoch 139/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9826\nEpoch 140/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0752 - accuracy: 0.9822\nEpoch 141/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9843\nEpoch 142/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0779 - accuracy: 0.9848\nEpoch 143/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0825 - accuracy: 0.9796\nEpoch 144/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0848 - accuracy: 0.9807\nEpoch 145/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9805\nEpoch 146/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0757 - accuracy: 0.9853\nEpoch 147/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0808 - accuracy: 0.9815\nEpoch 148/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0854 - accuracy: 0.9816\nEpoch 149/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9855\nEpoch 150/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0812 - accuracy: 0.9825\nParam: 0.000100, Train: 0.984, Test: 0.965\n[INFO] training network...\nEpoch 1/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.7306 - accuracy: 0.5421\nEpoch 2/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.6780 - accuracy: 0.6370\nEpoch 3/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6868\nEpoch 4/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5513 - accuracy: 0.7237\nEpoch 5/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.7362\nEpoch 6/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.5280 - accuracy: 0.7654\nEpoch 7/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.5485 - accuracy: 0.7739\nEpoch 8/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4446 - accuracy: 0.8098\nEpoch 9/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.8164\nEpoch 10/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.8003\nEpoch 11/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.4098 - accuracy: 0.8249\nEpoch 12/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.3699 - accuracy: 0.8500\nEpoch 13/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3629 - accuracy: 0.8558\nEpoch 14/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3646 - accuracy: 0.8545\nEpoch 15/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3605 - accuracy: 0.8571\nEpoch 16/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.3336 - accuracy: 0.8563\nEpoch 17/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3054 - accuracy: 0.8793\nEpoch 18/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2984 - accuracy: 0.8752\nEpoch 19/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3133 - accuracy: 0.8833\nEpoch 20/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2736 - accuracy: 0.8993\nEpoch 21/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8772\nEpoch 22/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.8931\nEpoch 23/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2577 - accuracy: 0.9024\nEpoch 24/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2625 - accuracy: 0.8907\nEpoch 25/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.8950\nEpoch 26/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2557 - accuracy: 0.8987\nEpoch 27/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2460 - accuracy: 0.9081\nEpoch 28/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.9081\nEpoch 29/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2201 - accuracy: 0.9172\nEpoch 30/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2296 - accuracy: 0.9149\nEpoch 31/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2303 - accuracy: 0.9107\nEpoch 32/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9216: 0s - loss: 0.1818 - ac\nEpoch 33/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2133 - accuracy: 0.9239\nEpoch 34/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2103 - accuracy: 0.9219\nEpoch 35/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9209\nEpoch 36/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2011 - accuracy: 0.9212\nEpoch 37/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9166\nEpoch 38/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.9259\nEpoch 39/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1953 - accuracy: 0.9271\nEpoch 40/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2161 - accuracy: 0.9259\nEpoch 41/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1847 - accuracy: 0.9225\nEpoch 42/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2004 - accuracy: 0.9294\nEpoch 43/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1939 - accuracy: 0.9304\nEpoch 44/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.9176\nEpoch 45/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1724 - accuracy: 0.9323\nEpoch 46/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1733 - accuracy: 0.9449\nEpoch 47/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1970 - accuracy: 0.9303\nEpoch 48/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.1819 - accuracy: 0.9315\nEpoch 49/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1713 - accuracy: 0.9346\nEpoch 50/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1911 - accuracy: 0.9362\nEpoch 51/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1722 - accuracy: 0.9407\nEpoch 52/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1722 - accuracy: 0.9356\nEpoch 53/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1699 - accuracy: 0.9403\nEpoch 54/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1535 - accuracy: 0.9454\nEpoch 55/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1566 - accuracy: 0.9423\nEpoch 56/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1743 - accuracy: 0.9343\nEpoch 57/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9400\nEpoch 58/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1562 - accuracy: 0.9467\nEpoch 59/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.9479\nEpoch 60/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1543 - accuracy: 0.9381\nEpoch 61/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1513 - accuracy: 0.9424\nEpoch 62/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1456 - accuracy: 0.9490\nEpoch 63/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9588\nEpoch 64/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1405 - accuracy: 0.9506\nEpoch 65/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1484 - accuracy: 0.9457\nEpoch 66/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1594 - accuracy: 0.9321\nEpoch 67/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1459 - accuracy: 0.9513\nEpoch 68/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1583 - accuracy: 0.9448\nEpoch 69/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9606\nEpoch 70/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1220 - accuracy: 0.9551\nEpoch 71/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1244 - accuracy: 0.9604\nEpoch 72/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.1090 - accuracy: 0.9634\nEpoch 73/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1138 - accuracy: 0.9663\nEpoch 74/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1085 - accuracy: 0.9627\nEpoch 75/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1105 - accuracy: 0.9654\nEpoch 76/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1128 - accuracy: 0.9678\nEpoch 77/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1159 - accuracy: 0.9537\nEpoch 78/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1140 - accuracy: 0.9588\nEpoch 79/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1050 - accuracy: 0.9641\nEpoch 80/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.1056 - accuracy: 0.9634\nEpoch 81/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0934 - accuracy: 0.9744\nEpoch 82/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1051 - accuracy: 0.9657\nEpoch 83/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0947 - accuracy: 0.9758\nEpoch 84/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0913 - accuracy: 0.9734\nEpoch 85/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 0.9698\nEpoch 86/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0816 - accuracy: 0.9789\nEpoch 87/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0827 - accuracy: 0.9786\nEpoch 88/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0919 - accuracy: 0.9686\nEpoch 89/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1149 - accuracy: 0.9658\nEpoch 90/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0834 - accuracy: 0.9731\nEpoch 91/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0919 - accuracy: 0.9713\nEpoch 92/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0902 - accuracy: 0.9733\nEpoch 93/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9819\nEpoch 94/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0767 - accuracy: 0.9793\nEpoch 95/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0906 - accuracy: 0.9766\nEpoch 96/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0793 - accuracy: 0.9763\nEpoch 97/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0815 - accuracy: 0.9754\nEpoch 98/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0792 - accuracy: 0.9740\nEpoch 99/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0805 - accuracy: 0.9761\nEpoch 100/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0850 - accuracy: 0.9760\nEpoch 101/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0781 - accuracy: 0.9759\nEpoch 102/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0889 - accuracy: 0.9713\nEpoch 103/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0757 - accuracy: 0.9835\nEpoch 104/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0808 - accuracy: 0.9798\nEpoch 105/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0753 - accuracy: 0.9801\nEpoch 106/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9808\nEpoch 107/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0761 - accuracy: 0.9786\nEpoch 108/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0884 - accuracy: 0.9721\nEpoch 109/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0683 - accuracy: 0.9819\nEpoch 110/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0674 - accuracy: 0.9812\nEpoch 111/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0758 - accuracy: 0.9802\nEpoch 112/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0813 - accuracy: 0.9798\nEpoch 113/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0759 - accuracy: 0.9819\nEpoch 114/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0823 - accuracy: 0.9741\nEpoch 115/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0816 - accuracy: 0.9788\nEpoch 116/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9792\nEpoch 117/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0699 - accuracy: 0.9825\nEpoch 118/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0775 - accuracy: 0.9762\nEpoch 119/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0776 - accuracy: 0.9787\nEpoch 120/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0758 - accuracy: 0.9788\nEpoch 121/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.9822\nEpoch 122/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0799 - accuracy: 0.9764\nEpoch 123/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0761 - accuracy: 0.9753\nEpoch 124/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0685 - accuracy: 0.9831\nEpoch 125/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0671 - accuracy: 0.9818\nEpoch 126/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0681 - accuracy: 0.9804\nEpoch 127/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0709 - accuracy: 0.9835\nEpoch 128/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0673 - accuracy: 0.9791\nEpoch 129/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0734 - accuracy: 0.9815\nEpoch 130/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0734 - accuracy: 0.9789\nEpoch 131/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9857\nEpoch 132/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0710 - accuracy: 0.9799\nEpoch 133/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0686 - accuracy: 0.9846\nEpoch 134/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0907 - accuracy: 0.9747\nEpoch 135/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 0.9806\nEpoch 136/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0660 - accuracy: 0.9843\nEpoch 137/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0740 - accuracy: 0.9831\nEpoch 138/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9805\nEpoch 139/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0712 - accuracy: 0.9806\nEpoch 140/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9806\nEpoch 141/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9799\nEpoch 142/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0659 - accuracy: 0.9818\nEpoch 143/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9804\nEpoch 144/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0877 - accuracy: 0.9787\nEpoch 145/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0731 - accuracy: 0.9815\nEpoch 146/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.9770\nEpoch 147/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0694 - accuracy: 0.9791\nEpoch 148/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9818\nEpoch 149/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0703 - accuracy: 0.9841\nEpoch 150/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0591 - accuracy: 0.9857\nParam: 0.000050, Train: 0.982, Test: 0.967\n[INFO] training network...\nEpoch 1/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5471\nEpoch 2/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6406 - accuracy: 0.6511\nEpoch 3/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.7045\nEpoch 4/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.7154\nEpoch 5/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.6098 - accuracy: 0.7181\nEpoch 6/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7432\nEpoch 7/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.7680\nEpoch 8/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4582 - accuracy: 0.7829\nEpoch 9/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4122 - accuracy: 0.8293\nEpoch 10/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.4103 - accuracy: 0.8049\nEpoch 11/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3666 - accuracy: 0.8455\nEpoch 12/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8368\nEpoch 13/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8351\nEpoch 14/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3439 - accuracy: 0.8464\nEpoch 15/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8572\nEpoch 16/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3176 - accuracy: 0.8715\nEpoch 17/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.3225 - accuracy: 0.8690\nEpoch 18/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.8885\nEpoch 19/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2870 - accuracy: 0.8835\nEpoch 20/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.8895\nEpoch 21/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2918 - accuracy: 0.8713\nEpoch 22/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.8832: 0s - loss: 0.3121 \nEpoch 23/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8970\nEpoch 24/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.9037\nEpoch 25/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2074 - accuracy: 0.9184\nEpoch 26/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2001 - accuracy: 0.9157\nEpoch 27/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2093 - accuracy: 0.9158\nEpoch 28/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9208\nEpoch 29/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9190\nEpoch 30/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1861 - accuracy: 0.9268\nEpoch 31/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1886 - accuracy: 0.9210\nEpoch 32/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.1751 - accuracy: 0.9316\nEpoch 33/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1892 - accuracy: 0.9312\nEpoch 34/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9303\nEpoch 35/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1660 - accuracy: 0.9407\nEpoch 36/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.1587 - accuracy: 0.9450\nEpoch 37/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1689 - accuracy: 0.9240\nEpoch 38/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1694 - accuracy: 0.9370\nEpoch 39/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1567 - accuracy: 0.9404\nEpoch 40/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1568 - accuracy: 0.9354\nEpoch 41/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1572 - accuracy: 0.9341\nEpoch 42/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1556 - accuracy: 0.9398\nEpoch 43/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1447 - accuracy: 0.9441\nEpoch 44/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1394 - accuracy: 0.9536\nEpoch 45/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1545 - accuracy: 0.9438\nEpoch 46/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1497 - accuracy: 0.9468\nEpoch 47/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1397 - accuracy: 0.9466\nEpoch 48/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1380 - accuracy: 0.9485\nEpoch 49/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1304 - accuracy: 0.9487\nEpoch 50/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1459 - accuracy: 0.9338\nEpoch 51/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1152 - accuracy: 0.9579\nEpoch 52/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1320 - accuracy: 0.9514\nEpoch 53/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.9606\nEpoch 54/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1417 - accuracy: 0.9434\nEpoch 55/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1050 - accuracy: 0.9584\nEpoch 56/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.9585\nEpoch 57/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9675\nEpoch 58/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0934 - accuracy: 0.9651\nEpoch 59/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1118 - accuracy: 0.9566\nEpoch 60/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1027 - accuracy: 0.9582\nEpoch 61/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0947 - accuracy: 0.9599\nEpoch 62/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0875 - accuracy: 0.9687\nEpoch 63/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0952 - accuracy: 0.9627\nEpoch 64/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0888 - accuracy: 0.9702\nEpoch 65/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.1102 - accuracy: 0.9610\nEpoch 66/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0949 - accuracy: 0.9661\nEpoch 67/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 0.9662\nEpoch 68/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0872 - accuracy: 0.9684\nEpoch 69/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0953 - accuracy: 0.9656\nEpoch 70/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0859 - accuracy: 0.9665\nEpoch 71/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0978 - accuracy: 0.9624\nEpoch 72/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0789 - accuracy: 0.9709\nEpoch 73/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0834 - accuracy: 0.9727\nEpoch 74/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.9738\nEpoch 75/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0907 - accuracy: 0.9652\nEpoch 76/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0812 - accuracy: 0.9710\nEpoch 77/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0845 - accuracy: 0.9691\nEpoch 78/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0698 - accuracy: 0.9759\nEpoch 79/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9756\nEpoch 80/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0767 - accuracy: 0.9725\nEpoch 81/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0753 - accuracy: 0.9757\nEpoch 82/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0738 - accuracy: 0.9748\nEpoch 83/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9751\nEpoch 84/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0641 - accuracy: 0.9760\nEpoch 85/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0779 - accuracy: 0.9729\nEpoch 86/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0701 - accuracy: 0.9768\nEpoch 87/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0667 - accuracy: 0.9768\nEpoch 88/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0669 - accuracy: 0.9770\nEpoch 89/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0716 - accuracy: 0.9804\nEpoch 90/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9765\nEpoch 91/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0765 - accuracy: 0.9736\nEpoch 92/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0691 - accuracy: 0.9796\nEpoch 93/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0717 - accuracy: 0.9759\nEpoch 94/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0644 - accuracy: 0.9838\nEpoch 95/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 0.9849\nEpoch 96/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0646 - accuracy: 0.9791\nEpoch 97/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9779\nEpoch 98/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.9776\nEpoch 99/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0677 - accuracy: 0.9772\nEpoch 100/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0667 - accuracy: 0.9728\nEpoch 101/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0603 - accuracy: 0.9802\nEpoch 102/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 0.9807\nEpoch 103/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9817\nEpoch 104/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.0565 - accuracy: 0.9810\nEpoch 105/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0652 - accuracy: 0.9801\nEpoch 106/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 0.9810\nEpoch 107/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 0.9806\nEpoch 108/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9834: 0s - loss: 0.0539 - \nEpoch 109/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0540 - accuracy: 0.9814\nEpoch 110/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0568 - accuracy: 0.9795\nEpoch 111/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0528 - accuracy: 0.9812\nEpoch 112/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9829\nEpoch 113/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.9817\nEpoch 114/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0647 - accuracy: 0.9783\nEpoch 115/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0595 - accuracy: 0.9838\nEpoch 116/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9851\nEpoch 117/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.9771\nEpoch 118/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9815\nEpoch 119/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9802\nEpoch 120/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0476 - accuracy: 0.9833\nEpoch 121/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9784\nEpoch 122/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.9833\nEpoch 123/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0553 - accuracy: 0.9798\nEpoch 124/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9787\nEpoch 125/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.9763\nEpoch 126/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9748\nEpoch 127/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0655 - accuracy: 0.9744\nEpoch 128/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0551 - accuracy: 0.9828\nEpoch 129/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.9835\nEpoch 130/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9805\nEpoch 131/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0486 - accuracy: 0.9851\nEpoch 132/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0615 - accuracy: 0.9755\nEpoch 133/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0599 - accuracy: 0.9787\nEpoch 134/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9782\nEpoch 135/150\n297/297 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9792\nEpoch 136/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9801\nEpoch 137/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9857\nEpoch 138/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0463 - accuracy: 0.9854\nEpoch 139/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0590 - accuracy: 0.9817\nEpoch 140/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0537 - accuracy: 0.9841\nEpoch 141/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0592 - accuracy: 0.9806\nEpoch 142/150\n297/297 [==============================] - 1s 2ms/step - loss: 0.0552 - accuracy: 0.9804\nEpoch 143/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0593 - accuracy: 0.9785\nEpoch 144/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9858\nEpoch 145/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 0.9810\nEpoch 146/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9797\nEpoch 147/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9808\nEpoch 148/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9827\nEpoch 149/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9841\nEpoch 150/150\n297/297 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9812\nParam: 0.000010, Train: 0.982, Test: 0.965\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Plotting train and test accuracies depending on regularization parameters","metadata":{}},{"cell_type":"code","source":"plt.semilogx(regul, all_train, label='train', marker='o')\nplt.semilogx(regul, all_test, label='test', marker='o')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building dataframe for finding high accuracy and low difference between train and test accuracies. It will define the best regularization parameter","metadata":{}},{"cell_type":"code","source":"all_train_array = np.array(all_train)\nall_test_array = np.array(all_test)\naccuracy_frame = pd.DataFrame({\n                        'regularization' : regul,\n                        'accuracy_on_train': all_train_array*100, \n                         'accuracy_on_test': all_test_array*100,\n                         'accuracy_difference': abs((all_train_array - all_test_array)*100)\n                              })\nacc=accuracy_frame.sort_values(by=['accuracy_difference'])\nacc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best accuracy is 97% and best parameter regularization is 0.005","metadata":{}},{"cell_type":"code","source":"#best model was models[1], 2nd model\n#testing the 2nd model on the test data\npredict = (models[1].predict(testX) > 0.5).astype(\"int32\")\n\n#functions that give the metrics about the testing accuracy, recall, f1 score, etc.\nprint(classification_report(testY,predict))\nprint(confusion_matrix(testY, predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy:',(accuracy_score(testY, predict))*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DNN with Dropout","metadata":{}},{"cell_type":"markdown","source":"Let's build model with Dropout regularization technique for reducing overfitting","metadata":{}},{"cell_type":"markdown","source":"Setting the model","metadata":{}},{"cell_type":"code","source":"all_train, all_test = list(), list()\nmodels = list()\nparameters = c\nfor i in parameters:\n    model = Sequential()\n    model.add(Dense(input_shape=(trainX.shape[1],), units=256, \n                                activation=\"relu\"))\n    model.add(Dropout(i))\n    model.add(Dense(128, activation=\"relu\"))\n    model.add(Dropout(i))\n    model.add(Dense(64, activation=\"relu\"))\n    model.add(Dense(64, activation=\"relu\"))\n    model.add(Dense(32, activation=\"relu\"))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    #compile the model using Adam as an optimizer and \n    #a binary cross entropy as a loss function\n    print(\"[INFO] training network...\")\n    opt = Adam(lr=INIT_LR)\n    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n    callbacks = [ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=0.00001)] \n    history_model = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH, callbacks=callbacks, verbose=1)\n    models.append(model)\n    # evaluate the model using train and test data\n    _, train_acc = model.evaluate(trainX, trainY, verbose=0)\n    _, test_acc = model.evaluate(testX, testY, verbose=0)\n    print('Param: %f, Train: %.3f, Test: %.3f' % (param, train_acc, test_acc))\n    all_train.append(train_acc)\n    all_test.append(test_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting train and test accuracies depending on Dropout parameter","metadata":{}},{"cell_type":"code","source":"plt.semilogx(parameters, all_train, label='train', marker='o')\nplt.semilogx(parameters, all_test, label='test', marker='o')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_train_array = np.array(all_train)\nall_test_array = np.array(all_test)\naccuracy_frame = pd.DataFrame({\n                        'parameter' : parameters,\n                        'accuracy_on_train': all_train_array*100, \n                         'accuracy_on_test': all_test_array*100,\n                         'accuracy_difference': abs((all_train_array - all_test_array)*100)\n                              })\nacc=accuracy_frame.sort_values(by=['accuracy_difference'])\nacc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = (models[3].predict(testX) > 0.5).astype(\"int32\")\nprint(classification_report(testY,predict))\nprint(confusion_matrix(testY, predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy:',(accuracy_score(testY, predict))*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}